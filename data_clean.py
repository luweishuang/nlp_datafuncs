# -*- coding: utf-8 -*-

import re
import json
import cn2an      # é˜¿æ‹‰ä¼¯æ•°å­— <=> ä¸­æ–‡æ•°å­—
import string
import unicodedata


all_flag = string.punctuation + u'â€œã€Šã€‹ã€Œã€ã€ã€Â·â€”â–¡ã€ˆã€‰â€¢â€™â—â€˜Ã—â€ãƒ»âˆ«,?!.â™ª:â¦†â¦†â•®â•­ã€œğŸ˜‚ğŸ‘ğŸ’¨âœ¨â—¤â—¢â˜€â‚¬ğŸ˜ğŸ™€î˜î„‡ãƒâ™¥â˜…â‹¯â‹¯Ïƒâ‰ªâ‰«â™¡â¢â—Šîˆ.|:â€”.î…Œâ†“âˆ©'
pattern_flag = re.compile('[%s]' % re.escape(all_flag))
alpha_char = string.punctuation + u'abcdefghijklmnopqrstuvwxyz'
pattern_alpha = re.compile('[%s]' % re.escape(alpha_char))

trantab = str.maketrans('ï¼Œã€‚ï¼ï¼Ÿã€ã€‘ï¼ˆï¼‰ã€”ã€•ï¼…ï¼ƒï¼ ï¼†ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ã€', ',.!?[]()[]%#@&1234567890,')


def is_all_chinese1(strs):
    for _char in strs:
        if not '\u4e00' <= _char <= '\u9fa5':
            return False
    return True


def is_all_chinese2(strs):
    if all(map(lambda c: '\u4e00' <= c <= '\u9fa5', strs)):
        return True
    return False


def filter_emoji(desstr,restr=''):
    try:
        co = re.compile(u'[\U00010000-\U0010ffff]')
    except re.error:
        co = re.compile(u'[\uD800-\uDBFF][\uDC00-\uDFFF]')
    return co.sub(restr, desstr)


def data_clean1(str_in):
    text_a = str_in.strip().replace(" ", "").replace("alink", "").replace("Â°C", "åº¦")
    text_b = filter_emoji(text_a, restr='')
    text_1 = unicodedata.normalize('NFKC', text_b.lower().replace(" ", ""))      # ä¸­æ–‡æ ‡ç‚¹è½¬æ¢ä¸ºè‹±æ–‡æ ‡ç‚¹
    text_2 = text_1.translate(trantab)  # æ¼ç½‘ä¹‹é±¼æ‰‹åŠ¨ä¿®æ”¹å¯¹åº”
    text_3 = re.sub(u"\\(.*?\\)|\\{.*?}|\\[.*?]", "", text_2)   # å»æ‰å°æ‹¬å·(), {}, []é‡Œçš„å†…å®¹
    text_4 = cn2an.transform(text_3, "an2cn")                   # é˜¿æ‹‰ä¼¯æ•°å­—è½¬ä¸­æ–‡
    text_5 = pattern_flag.sub(u'', text_4)                      # å»æ‰æ ‡ç‚¹ç¬¦å·
    if not is_all_chinese1(text_5):
        text_6 = pattern_alpha.sub(u'', text_5)
        if not is_all_chinese1(text_6):
            # print(text_5)
            return ""
    return text_3


if __name__ == "__main__":
    # s = "å›å®¶æ¥ç¬¬ä¸€é¡¿é¤å¤ªå·´é€‚äº†æ’’îƒîŒ¼îî€îŒ¿"           # "çŒ«å„¿æœ¬â¤â¤å¥½æ¼‚äº®çš„å›½åº¦"      "ğŸ¤å”±å§ä½ è¿˜æˆ‘å¦»å­ğŸ˜­ğŸ˜­ğŸ˜­"   å›å®¶æ¥ç¬¬ä¸€é¡¿é¤å¤ªå·´é€‚äº†æ’’îƒîŒ¼îî€îŒ¿     çŒ«å„¿æœ¬â¤â¤å¥½æ¼‚äº®çš„å›½åº¦
    # print(filter_emoji(s, restr=''))
    # exit()

    data_file = "/home/psc/Desktop/code/conv/sentence-transformers/examples/training/quora_duplicate_questions/data/STC.json"
    train_samples = []
    discard_num = 0
    use_num = 0
    with open(data_file, "r", encoding="utf-8") as f:
        dataset = json.loads(f.read())
        for k, v in dataset.items():
            for cur_dialg in v:
                query_sent = data_clean1(cur_dialg[0])
                content_sent = data_clean1(cur_dialg[1])
                if len(query_sent)==0 or len(content_sent)==0:
                    discard_num += 1
                    continue
                else:
                    use_num += 1
    print("discard_num / all_num = %d / %d" % (discard_num, discard_num + use_num))









'''
data/STC.json
discard_num / all_num = 114146 / 4414798 == 2.5%
'''